{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "SA_Ex2_v2.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU",
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QSeXzYiT8Q2y",
    "colab_type": "text"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "HibgsBc0tVH8",
    "colab_type": "code",
    "outputId": "8237cb7d-7187-46c2-e443-585264c72596",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import tensorflow.compat.v2 as tf \n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow.keras as keras\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "tf.version.VERSION"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8IrENo7qBw8R",
    "colab_type": "code",
    "outputId": "2d7f7909-e2df-4650-c1a4-17f5f030c970",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    }
   },
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-wbn0DtgYvUX",
    "colab_type": "text"
   },
   "source": [
    "# Preprocessing methods"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-tKJrVsRYv7c",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "stop_words = set(stopwords.words(\"english\")) \n",
    "def remove_stop_words(text):\n",
    "    text = [word for word in text.split() if not word in stop_words]\n",
    "    text = \" \".join(text)\n",
    "    return text\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    text = re.sub(r'[^\\w\\s]','',text)\n",
    "    return text"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "C67w3wgl9pD-",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "## Tokenization and padding\n",
    "def vectorize_text(_data, _dict_size: int, _max_length: int):   \n",
    "    max_dictionary_size = _dict_size\n",
    "    tokenizer = Tokenizer(num_words=max_dictionary_size)\n",
    "    tokenizer.fit_on_texts(_data['SentimentText'])\n",
    "    list_tokenized_train = tokenizer.texts_to_sequences(_data['SentimentText'])\n",
    "    print(f\"Max length = {_max_length}\")\n",
    "    X_t = pad_sequences(list_tokenized_train, maxlen=_max_length, padding='post')\n",
    "    print(len(tokenizer.index_word))\n",
    "    return X_t"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "eUmkEE4kP62j",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "def join_phrases(text: str, ngrams):\n",
    "    return \" \".join(ngrams[text.split()])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1VIv9Pg9__Yr",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "## Create phrases of bigrams / trigrams and vectorize text + padding \n",
    "def vectorize_ngrams(_data, _dict_size: int, _max_length: int, ngram_size: int, threshold: int):\n",
    "    all_reviews = _data['SentimentText'].values\n",
    "    all_reviews = np.array(list(map(lambda x: x.split(), all_reviews)))\n",
    "\n",
    "    ngrams = Phrases(sentences=all_reviews, threshold=threshold)\n",
    "    if ngram_size == 3:\n",
    "        ngrams = Phrases(sentences=ngrams[all_reviews])\n",
    "    elif ngram_size > 3:\n",
    "        raise ValueError(\"Not implemented for this ngram size!\")\n",
    "    phraser = Phraser(ngrams)\n",
    "    text_ngrams = _data['SentimentText'].apply(lambda x: join_phrases(x, phraser))\n",
    "    tokenizer = Tokenizer(num_words=_dict_size, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^`{|}~\\t\\n')\n",
    "    tokenizer.fit_on_texts(text_ngrams)\n",
    "    list_tokenized_train = tokenizer.texts_to_sequences(text_ngrams)\n",
    "    X_t = pad_sequences(list_tokenized_train, maxlen=_max_length, padding='post')\n",
    "    len(tokenizer.index_word)\n",
    "    return X_t"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QvBnlEEWcYa0",
    "colab_type": "text"
   },
   "source": [
    "# Process metod"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "yCothxuAYz-U",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "## Text preprocessing + model training + evaluation\n",
    "def preprocess_train(data, functions, classifier, _dict_size, word_ngrams=1, words_per_review=None, threshold=10):\n",
    "    # copy data frame\n",
    "    _data = pd.DataFrame(data['SentimentText'])  # Reviews\n",
    "    y = data['Sentiment']  # Sentiment\n",
    "    # apply preprocessing methods\n",
    "    for function in functions:\n",
    "        _data['SentimentText'] = _data['SentimentText'].apply(lambda x: function(x))\n",
    "    _row_sizes = _data['SentimentText'].str.split().str.len()\n",
    "    print(f\"Words count: {pd.Series.sum(_row_sizes)}\")\n",
    "    print(_data)\n",
    "    print(f\"Words ngrams: {word_ngrams}\")\n",
    "    # Get longest review (words)\n",
    "    _data['review_lenght'] = np.array(list(map(lambda x: len(x.split()), _data['SentimentText'])))\n",
    "    # set max review length\n",
    "    if words_per_review is None:\n",
    "        max_length = _data['review_lenght'].max()\n",
    "    else:\n",
    "        max_length = words_per_review\n",
    "    # Vectorize reviews\n",
    "    if word_ngrams == 1:\n",
    "        X_data = vectorize_text(_data, _dict_size, max_length)\n",
    "    else:\n",
    "        X_data = vectorize_ngrams(_data, _dict_size, max_length, word_ngrams, threshold)\n",
    "    # train and evaluate\n",
    "    result = classifier(X_data, y, max_length)\n",
    "    return result   # return average accuracy"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nmBejzIk8YVk",
    "colab_type": "text"
   },
   "source": [
    "# YELP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YdKV2jWAtXm0",
    "colab_type": "text"
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "J2b9cEav8aKL",
    "colab_type": "code",
    "outputId": "f0c2fe77-5213-427b-f412-d314be8e9664",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    }
   },
   "source": [
    "path = \"../data/yelp_labelled.txt\"\n",
    "yelpData = pd.read_csv(path, sep='\\t', header=0, encoding=\"utf-8\")\n",
    "row_sizes = yelpData['SentimentText'].str.split().str.len()\n",
    "yelpData['SentimentText'] = yelpData['SentimentText'].str.lower()\n",
    "print(f\"Words count: {pd.Series.sum(row_sizes)}\")\n",
    "max_dictionary_size = 2071\n",
    "yelpData"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load lemmatized Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "eqV1yTL2taOJ",
    "colab_type": "code",
    "outputId": "6f00c15a-607a-42e6-e2b4-743ce203704d",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    }
   },
   "source": [
    "path = \"../data/YelpLemmatized.txt\"\n",
    "yelpDataLem = pd.read_csv(path, sep='\\t', header=0, encoding=\"utf-8\")\n",
    "row_sizes = yelpDataLem['SentimentText'].str.split().str.len()\n",
    "print(f\"Words count: {pd.Series.sum(row_sizes)}\")\n",
    "yelpDataLem"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "co1jB1bMFcG4",
    "colab_type": "text"
   },
   "source": [
    "## LSTM create and train model for yelp"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "67gLJwPaFWR8",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "def lstm_yelp(_data, _targets, max_length): \n",
    "    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    fold = 0\n",
    "    results = list()\n",
    "\n",
    "    early_stopping = keras.callbacks.EarlyStopping(monitor='val_accuracy',\n",
    "                                        min_delta=0,\n",
    "                                        patience=4,\n",
    "                                        verbose=1,\n",
    "                                        mode='auto',\n",
    "                                        restore_best_weights=True)\n",
    "\n",
    "    for train, test in kfold.split(_data, _targets):\n",
    "        print(f\"******* Fold {fold + 1} ***********\")\n",
    "        model = keras.models.Sequential([\n",
    "            keras.layers.Embedding(max_dictionary_size, 16, input_length=max_length),\n",
    "            keras.layers.Bidirectional(keras.layers.LSTM(16, return_sequences=True)),\n",
    "            keras.layers.GlobalMaxPooling1D(),\n",
    "            keras.layers.Dense(16),\n",
    "            keras.layers.Dense(1, activation=\"sigmoid\")                                \n",
    "        ])\n",
    "\n",
    "        model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])\n",
    "        model.fit(_data[train],_targets[train], batch_size=8, epochs=10, verbose=0, validation_data=(_data[test], _targets[test]), callbacks=[early_stopping])\n",
    "        scores = model.evaluate(_data[test], _targets[test])\n",
    "        results.append(scores[1])\n",
    "        fold += 1\n",
    "    avg = sum(results)/fold * 100\n",
    "    print(f\"Average accuracy = {avg:0.2f} %\")\n",
    "    return avg"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1uex1EAyfYDi",
    "colab_type": "text"
   },
   "source": [
    "## CNN create and train model for yelp"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "GFk9ObZmfa7A",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "def cnn_yelp(_data, _targets, max_length):\n",
    "    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    fold = 0\n",
    "    results = list()\n",
    "    filters = 64\n",
    "    kernel_size = 3\n",
    "    early_stopping = keras.callbacks.EarlyStopping(monitor='val_accuracy',\n",
    "                                    min_delta=0,\n",
    "                                    patience=4,\n",
    "                                    verbose=1,\n",
    "                                    mode='auto',\n",
    "                                    restore_best_weights=True)\n",
    "    for train, test in kfold.split(_data,_targets):\n",
    "        print(f\"******* Fold {fold + 1} ***********\")\n",
    "        model = keras.models.Sequential([\n",
    "            keras.layers.Embedding(max_dictionary_size, 16, input_length=max_length),\n",
    "            keras.layers.Conv1D(filters, kernel_size, activation=\"relu\"),\n",
    "            keras.layers.GlobalMaxPooling1D(),\n",
    "            keras.layers.Dense(64),\n",
    "            keras.layers.Activation(\"relu\"),\n",
    "            keras.layers.Dense(1, activation=\"sigmoid\")                                \n",
    "        ])\n",
    "\n",
    "        model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])\n",
    "        model.fit(_data[train],_targets[train], batch_size=8, epochs=10, verbose=0, validation_data=(_data[test], _targets[test]), callbacks=[early_stopping])\n",
    "        scores = model.evaluate(_data[test], _targets[test])\n",
    "        results.append(scores[1])\n",
    "        fold += 1\n",
    "    avg = sum(results)/fold * 100\n",
    "    print(f\"Average accuracy = {avg:0.2f} %\")\n",
    "    return avg"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IPPK7iKT9naI",
    "colab_type": "text"
   },
   "source": [
    "## Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "MY2vb4AJab-4",
    "colab_type": "code",
    "outputId": "5bfac1e9-dea3-48a6-96fc-0c69ee8d43da",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 733
    }
   },
   "source": [
    "lstm_result = preprocess_train(yelpData, [remove_punctuation], lstm_yelp, max_dictionary_size)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "NTQdNlD0liE4",
    "colab_type": "code",
    "outputId": "2ff74611-87ae-4f26-86ff-08d5d81484c6",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 733
    }
   },
   "source": [
    "cnn_result = preprocess_train(yelpData, [remove_punctuation], cnn_yelp, max_dictionary_size)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a_XBtT9asrK3",
    "colab_type": "text"
   },
   "source": [
    "## Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SPxaTVhqtGWv",
    "colab_type": "code",
    "outputId": "07af7513-bfb1-4f70-ff21-3d260fcb411e",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 733
    }
   },
   "source": [
    "lstm_result = preprocess_train(yelpData, [remove_stop_words], lstm_yelp, max_dictionary_size)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6gXuPYH_tKIO",
    "colab_type": "code",
    "outputId": "a5e956a3-ee68-40ec-8f16-2d4d808b46a8",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 733
    }
   },
   "source": [
    "cnn_result = preprocess_train(yelpData, [remove_stop_words], cnn_yelp, max_dictionary_size)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XMiBfDKAtDEj",
    "colab_type": "text"
   },
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "r-vXJtBGtTEs",
    "colab_type": "code",
    "outputId": "e29f1b69-9970-48aa-d517-d13aff64d24c",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 733
    }
   },
   "source": [
    "lstm_result = preprocess_train(yelpDataLem, [], lstm_yelp, 1771)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "61ZSpn49t6tV",
    "colab_type": "code",
    "outputId": "761e6582-991d-458e-8103-6f8250ade301",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 697
    }
   },
   "source": [
    "cnn_result = preprocess_train(yelpDataLem, [], cnn_yelp, 1771)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "olXR9mD6xkDk",
    "colab_type": "text"
   },
   "source": [
    "## Remove stop words AND remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qEcSH1Hsxkue",
    "colab_type": "code",
    "outputId": "2884fae9-200c-4fb0-db43-d89b4a604f51",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 660
    }
   },
   "source": [
    "lstm_result = preprocess_train(yelpDataLem, [remove_stop_words, remove_punctuation], lstm_yelp, 1691)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "YoVtM0V5xxxY",
    "colab_type": "code",
    "outputId": "4bb5bf72-864d-4cbd-b853-3b29aca5285e",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 697
    }
   },
   "source": [
    "cnn_result = preprocess_train(yelpData, [remove_stop_words, remove_punctuation], cnn_yelp, 1691)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JoDnFDCux2Cd",
    "colab_type": "text"
   },
   "source": [
    "## Remove stop words AND Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "h9jprL1nx4on",
    "colab_type": "code",
    "outputId": "7fb0b846-6dd6-49dc-fde2-3524e4fab2f5",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 697
    }
   },
   "source": [
    "lstm_result = preprocess_train(yelpDataLem, [remove_stop_words], lstm_yelp, 1693)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8PEk1y8px9Td",
    "colab_type": "code",
    "outputId": "9ab4d3ae-0ab8-4173-bbac-a2a7b740c2bc",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 733
    }
   },
   "source": [
    "lstm_result = preprocess_train(yelpDataLem, [remove_stop_words], lstm_yelp, 1693)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6JyxVLGTyiL2",
    "colab_type": "text"
   },
   "source": [
    "## N-GRAMS"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "TKQe26CxyjHq",
    "colab_type": "code",
    "outputId": "7beb3083-a341-484a-ad2b-a94812a13913",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    }
   },
   "source": [
    "for i in range(1, 4):\n",
    "    for j in [10, 20, 40]:\n",
    "        scores = preprocess_train(yelpDataLem, [], lstm_yelp, max_dictionary_size, word_ngrams=i, threshold=j)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xr_DedhYzGwB",
    "colab_type": "code",
    "outputId": "e465c9dc-5ee5-4b37-c41e-43ec2b760e96",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    }
   },
   "source": [
    "for i in range(1, 4):\n",
    "    for j in [10, 20, 40]:\n",
    "        scores = preprocess_train(yelpDataLem, [], cnn_yelp, 1771, word_ngrams=i, threshold=j)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eg8L177zgN2f",
    "colab_type": "text"
   },
   "source": [
    "# IMDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zy6KxJaWDWFQ",
    "colab_type": "text"
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xyAHbiZwgRHB",
    "colab_type": "code",
    "outputId": "e2015595-6802-4ccf-9e4f-a24363c79306",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    }
   },
   "source": [
    "path = \"../data/imdb_50k.tsv\"\n",
    "imdbData = pd.read_csv(path, sep='\\t', header=0, encoding=\"utf-8\", doublequote=False, escapechar=\"\\\\\")\n",
    "imdbData = imdbData.drop(['id'], axis=1)\n",
    "row_sizes = imdbData['SentimentText'].str.split().str.len()\n",
    "imdbData['SentimentText'] = imdbData['SentimentText'].str.lower()\n",
    "print(f\"Words count: {pd.Series.sum(row_sizes)}\")\n",
    "max_dictionary_size = 10000\n",
    "max_review_words = 400\n",
    "imdbData"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load lemmatized data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "l9CIPuzxDYQs",
    "colab_type": "code",
    "outputId": "d1fc202c-257f-4fe7-fc70-686e612f0f2f",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    }
   },
   "source": [
    "path = \"../data/Imdb50KLemmatized.tsv\"\n",
    "imdbDataLem = pd.read_csv(path, sep='\\t', header=0, encoding=\"utf-8\", doublequote=False, escapechar=\"\\\\\")\n",
    "imdbDataLem = imdbDataLem.drop(['id'], axis=1)\n",
    "row_sizes = imdbDataLem['SentimentText'].str.split().str.len()\n",
    "imdbDataLem['SentimentText'] = imdbDataLem['SentimentText'].str.lower()\n",
    "print(f\"Words count: {pd.Series.sum(row_sizes)}\")\n",
    "imdbDataLem"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rpzx3Cjrgc9l",
    "colab_type": "text"
   },
   "source": [
    "## LSTM method for imdb"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4eYazwhbh42j",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "def lstm_imdb(_data, _targets, max_length): \n",
    "    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    fold = 0\n",
    "    results = list()\n",
    "\n",
    "    early_stopping = keras.callbacks.EarlyStopping(monitor='val_accuracy',\n",
    "                                        min_delta=0,\n",
    "                                        patience=3,\n",
    "                                        verbose=1,\n",
    "                                        mode='auto',\n",
    "                                        restore_best_weights=True)\n",
    "\n",
    "    for train, test in kfold.split(_data, _targets):\n",
    "        print(f\"******* Fold {fold + 1} ***********\")\n",
    "        model = keras.models.Sequential([\n",
    "            keras.layers.Embedding(max_dictionary_size, 16, input_length=max_length),\n",
    "            keras.layers.Bidirectional(keras.layers.LSTM(16, return_sequences=True)),\n",
    "            keras.layers.GlobalMaxPooling1D(),\n",
    "            keras.layers.Dense(16),\n",
    "            keras.layers.Dense(1, activation=\"sigmoid\")                        \n",
    "        ])\n",
    "\n",
    "        model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])\n",
    "        model.fit(_data[train],_targets[train], batch_size=64, epochs=10, verbose=0, validation_data=(_data[test], _targets[test]), callbacks=[early_stopping])\n",
    "        scores = model.evaluate(_data[test], _targets[test])\n",
    "        results.append(scores[1])\n",
    "        fold += 1\n",
    "    avg = sum(results)/fold * 100\n",
    "    print(f\"Average accuracy = {avg:0.2f} %\")\n",
    "    return avg"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HoOI0hJSgeJu",
    "colab_type": "text"
   },
   "source": [
    "## CNN method for imdb"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "yWXdFuzTgfLs",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "def cnn_imdb(_data, _targets, max_length):\n",
    "    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    fold = 0\n",
    "    results = list()\n",
    "    filters = 64\n",
    "    kernel_size = 3\n",
    "\n",
    "    early_stopping = keras.callbacks.EarlyStopping(monitor='val_accuracy',\n",
    "                                        min_delta=0,\n",
    "                                        patience=3,\n",
    "                                        verbose=1,\n",
    "                                        mode='auto',\n",
    "                                        restore_best_weights=True)\n",
    "\n",
    "    for train, test in kfold.split(_data,_targets):\n",
    "        print(f\"******* Fold {fold + 1} ***********\")\n",
    "        model = keras.models.Sequential([\n",
    "            keras.layers.Embedding(max_dictionary_size, 16, input_length=max_length),\n",
    "            keras.layers.Conv1D(filters, kernel_size, activation=\"relu\"),\n",
    "            keras.layers.GlobalMaxPooling1D(),\n",
    "            keras.layers.Dense(64),\n",
    "            keras.layers.Activation(\"relu\"),\n",
    "            keras.layers.Dense(1, activation=\"sigmoid\")                                  \n",
    "        ])\n",
    "\n",
    "        model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])\n",
    "        model.fit(_data[train],_targets[train], batch_size=32, epochs=10, verbose=0, validation_data=(_data[test], _targets[test]), callbacks=[early_stopping])\n",
    "        scores = model.evaluate(_data[test], _targets[test])\n",
    "        results.append(scores[1])\n",
    "        fold += 1\n",
    "    avg = sum(results)/fold * 100\n",
    "    print(f\"Average accuracy = {avg:0.2f} %\")\n",
    "    return avg"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mthjIcvVgX0l",
    "colab_type": "text"
   },
   "source": [
    "## Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "b9fufe8vFkBn",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "lstm_result = preprocess_train(imdbData, [remove_punctuation], lstm_imdb, max_dictionary_size, words_per_review=max_review_words)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8n2JxCWTGRTT",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "cnn_result = preprocess_train(imdbData, [remove_punctuation], cnn_imdb, max_dictionary_size, words_per_review=max_review_words)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S5RC92AmGan2",
    "colab_type": "text"
   },
   "source": [
    "## Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Vmq_IAr9GqtW",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "lstm_result = preprocess_train(imdbData, [remove_stop_words], lstm_imdb, max_dictionary_size, words_per_review=max_review_words)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "j_Lhd7u-GqTc",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "cnn_result = preprocess_train(imdbData, [remove_stop_words], cnn_imdb, max_dictionary_size, words_per_review=max_review_words)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rv9Xr7TAGdG5",
    "colab_type": "text"
   },
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "OrXpAllsGrfj",
    "colab_type": "code",
    "outputId": "efa8b97f-845d-43f7-8669-ac690a2cd234",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 752
    }
   },
   "source": [
    "lstm_result = preprocess_train(imdbDataLem, [], lstm_imdb, max_dictionary_size, words_per_review=max_review_words)\n",
    "lstm_result"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kOjmd9_0GrdS",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "cnn_result = preprocess_train(imdbDataLem, [], cnn_imdb, max_dictionary_size, words_per_review=max_review_words)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FtuG4HXNGj4G",
    "colab_type": "text"
   },
   "source": [
    "## Remove stopwords AND remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "t_iSAs9RGshd",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "lstm_result = preprocess_train(imdbData, [remove_stop_words, remove_punctuation], lstm_imdb, max_dictionary_size, words_per_review=max_review_words)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tdAGsZE8Gsd9",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "cnn_result = preprocess_train(imdbData, [remove_stop_words, remove_punctuation], cnn_imdb, max_dictionary_size, words_per_review=max_review_words)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JErxG-8PGfYf",
    "colab_type": "text"
   },
   "source": [
    "## Remove stopwords AND Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "o_T8JATAGuE4",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "lstm_result = preprocess_train(imdbDataLem, [remove_stop_words], lstm_imdb, max_dictionary_size, words_per_review=max_review_words)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JYxGGzW-GuBT",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "cnn_result = preprocess_train(imdbDataLem, [remove_stop_words], cnn_imdb, max_dictionary_size, words_per_review=max_review_words)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## N-grams"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i in range(1, 4):\n",
    "    for j in [10, 20, 40]:\n",
    "        print(f\"LTSM ngram {i}, threshold {j}\")\n",
    "        scores = preprocess_train(imdbDataLem, [], lstm_imdb, max_dictionary_size, words_per_review=max_review_words, word_ngrams=i, threshold=j)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i in range(1, 4):\n",
    "    for j in [10, 20, 40]:\n",
    "        print(f\"CNN ngram {i}, threshold {j}\")\n",
    "        scores = preprocess_train(imdbDataLem, [], cnn_imdb, max_dictionary_size, words_per_review=max_review_words, word_ngrams=i, threshold=j)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}